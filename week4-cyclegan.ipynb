{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":3304.661819,"end_time":"2024-02-20T22:33:52.985522","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-02-20T21:38:48.323703","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n## Overview of the Competition\nThe \"GAN Getting Started\" competition on Kaggle is designed to introduce participants to the world of Generative Adversarial Networks (GANs). In this competition, participants are tasked with generating new images that are stylistically similar to a provided dataset. The challenge focuses on assessing the ability of GANs to produce high-quality, realistic images that can be difficult to distinguish from the original data. This competition serves as a platform for data scientists, machine learning engineers, and enthusiasts to explore and innovate in the field of generative models, particularly GANs.\n","metadata":{}},{"cell_type":"markdown","source":"## Objective of the Notebook\nThis Jupyter Notebook aims to guide the reader through the process of developing a GAN model tailored to the competition's requirements. The primary objectives of this notebook are to:\n\n\n*   Provide a comprehensive understanding of the data and problem statement.\n*   Explore and implement data preprocessing techniques suitable for training GANs.\n*   Design, build, and train a GAN model from scratch.\n*   Evaluate the performance of the generated images against the competition's criteria.\n*  Share insights, challenges, and potential improvements for GAN models.","metadata":{}},{"cell_type":"markdown","source":"## Brief Introduction to Generative Adversarial Networks (GANs)\nGenerative Adversarial Networks (GANs) are a class of artificial intelligence algorithms used in unsupervised machine learning, implemented by a system of two neural networks contesting with each other in a zero-sum game framework. This technique was introduced by Ian Goodfellow and his colleagues in 2014 and has since been an area of active research and development.\n\nA GAN consists of two main parts: the generator and the discriminator. The generator's role is to create images (or other types of data) that resemble the real data as closely as possible. On the other hand, the discriminator's role is to distinguish between the generator's fake images and real images from the dataset. During training, these two networks are in a constant battle, with the generator trying to produce more and more realistic images, while the discriminator gets better at telling them apart from real images. The end goal is to train a generator that produces realistic images that the discriminator can no longer distinguish from real images, hence improving the generative model's performance.\n\nGANs have been used in various applications, including but not limited to, image generation, photo realistic image synthesis, style transfer, image super-resolution, and more. Their ability to generate new data from existing data makes them a powerful tool in the field of artificial intelligence and data science.","metadata":{}},{"cell_type":"markdown","source":"# Preparations & Installations","metadata":{"papermill":{"duration":0.009595,"end_time":"2024-02-20T21:38:51.039244","exception":false,"start_time":"2024-02-20T21:38:51.029649","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_datasets as tfds\n\nimport os\nimport time\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\nfrom kaggle_datasets import KaggleDatasets\n\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"papermill":{"duration":14.607322,"end_time":"2024-02-20T21:39:27.971634","exception":false,"start_time":"2024-02-20T21:39:13.364312","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:12:23.895397Z","iopub.execute_input":"2024-03-05T00:12:23.896045Z","iopub.status.idle":"2024-03-05T00:12:23.906686Z","shell.execute_reply.started":"2024-03-05T00:12:23.896013Z","shell.execute_reply":"2024-03-05T00:12:23.905751Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading dataset & Preprocessing","metadata":{"papermill":{"duration":0.008921,"end_time":"2024-02-20T21:39:27.990315","exception":false,"start_time":"2024-02-20T21:39:27.981394","status":"completed"},"tags":[]}},{"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path()","metadata":{"papermill":{"duration":0.541591,"end_time":"2024-02-20T21:39:28.540949","exception":false,"start_time":"2024-02-20T21:39:27.999358","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:12:27.990431Z","iopub.execute_input":"2024-03-05T00:12:27.990771Z","iopub.status.idle":"2024-03-05T00:12:28.573993Z","shell.execute_reply.started":"2024-03-05T00:12:27.990746Z","shell.execute_reply":"2024-03-05T00:12:28.573127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monet_files= tf.io.gfile.glob(str(GCS_PATH + '/monet_tfrec/*.tfrec'))\nphoto_files= tf.io.gfile.glob(str(GCS_PATH + '/photo_tfrec/*.tfrec'))","metadata":{"papermill":{"duration":0.386433,"end_time":"2024-02-20T21:39:28.937686","exception":false,"start_time":"2024-02-20T21:39:28.551253","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:12:31.106717Z","iopub.execute_input":"2024-03-05T00:12:31.107418Z","iopub.status.idle":"2024-03-05T00:12:32.828728Z","shell.execute_reply.started":"2024-03-05T00:12:31.107363Z","shell.execute_reply":"2024-03-05T00:12:32.827637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IMAGE_SIZE= [256,256]                                            \ndef decode_img(image):                                           \n    image= tf.image.decode_jpeg(image,channels= 3)\n    image= (tf.cast(image, tf.float32)/255)*2 -1\n    image= tf.reshape(image, shape= [*IMAGE_SIZE,3])\n    return image\n\ndef read_tfrec(example):\n    tfrec_format= {\n        'image_name': tf.io.FixedLenFeature([], tf.string),\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'target': tf.io.FixedLenFeature([], tf.string)\n    }\n    example= tf.io.parse_single_example(example, tfrec_format)\n    image= decode_img(example['image'])\n    return image","metadata":{"papermill":{"duration":0.020002,"end_time":"2024-02-20T21:39:28.967051","exception":false,"start_time":"2024-02-20T21:39:28.947049","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:12:35.010472Z","iopub.execute_input":"2024-03-05T00:12:35.011185Z","iopub.status.idle":"2024-03-05T00:12:35.017970Z","shell.execute_reply.started":"2024-03-05T00:12:35.011153Z","shell.execute_reply":"2024-03-05T00:12:35.017038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data(files):\n    data= tf.data.TFRecordDataset(files)\n    data= data.map(read_tfrec)\n    return data","metadata":{"papermill":{"duration":0.016282,"end_time":"2024-02-20T21:39:28.992383","exception":false,"start_time":"2024-02-20T21:39:28.976101","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:12:38.178790Z","iopub.execute_input":"2024-03-05T00:12:38.179150Z","iopub.status.idle":"2024-03-05T00:12:38.183961Z","shell.execute_reply.started":"2024-03-05T00:12:38.179120Z","shell.execute_reply":"2024-03-05T00:12:38.182747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"monet_data= load_data(monet_files).batch(1)\nphoto_data= load_data(photo_files).batch(1)","metadata":{"papermill":{"duration":0.840734,"end_time":"2024-02-20T21:39:29.842074","exception":false,"start_time":"2024-02-20T21:39:29.001340","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:12:40.615493Z","iopub.execute_input":"2024-03-05T00:12:40.615850Z","iopub.status.idle":"2024-03-05T00:12:41.449111Z","shell.execute_reply.started":"2024-03-05T00:12:40.615822Z","shell.execute_reply":"2024-03-05T00:12:41.448118Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_monet = monet_data\ntrain_photo = photo_data.take(300)\ntest_photo = photo_data.skip(300)","metadata":{"papermill":{"duration":0.021537,"end_time":"2024-02-20T21:39:29.873354","exception":false,"start_time":"2024-02-20T21:39:29.851817","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:12:43.086850Z","iopub.execute_input":"2024-03-05T00:12:43.087512Z","iopub.status.idle":"2024-03-05T00:12:43.102197Z","shell.execute_reply.started":"2024-03-05T00:12:43.087480Z","shell.execute_reply":"2024-03-05T00:12:43.101297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = 300\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256","metadata":{"papermill":{"duration":0.015562,"end_time":"2024-02-20T21:39:29.897990","exception":false,"start_time":"2024-02-20T21:39:29.882428","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:12:45.717555Z","iopub.execute_input":"2024-03-05T00:12:45.717929Z","iopub.status.idle":"2024-03-05T00:12:45.722443Z","shell.execute_reply.started":"2024-03-05T00:12:45.717899Z","shell.execute_reply":"2024-03-05T00:12:45.721441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_photo = next(iter(train_photo))\nsample_monet = next(iter(train_monet))","metadata":{"papermill":{"duration":0.923748,"end_time":"2024-02-20T21:39:30.830680","exception":false,"start_time":"2024-02-20T21:39:29.906932","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:12:48.034659Z","iopub.execute_input":"2024-03-05T00:12:48.034989Z","iopub.status.idle":"2024-03-05T00:12:53.836212Z","shell.execute_reply.started":"2024-03-05T00:12:48.034964Z","shell.execute_reply":"2024-03-05T00:12:53.835408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.subplot(121)\nplt.title('Photo')\nplt.imshow(sample_photo[0] * 0.5 + 0.5)\n\nplt.subplot(122)\nplt.title('Monet')\nplt.imshow(sample_monet[0] * 0.5 + 0.5)\n\nplt.show()","metadata":{"papermill":{"duration":0.411004,"end_time":"2024-02-20T21:39:31.251102","exception":false,"start_time":"2024-02-20T21:39:30.840098","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:13:03.295383Z","iopub.execute_input":"2024-03-05T00:13:03.295791Z","iopub.status.idle":"2024-03-05T00:13:03.727434Z","shell.execute_reply.started":"2024-03-05T00:13:03.295762Z","shell.execute_reply":"2024-03-05T00:13:03.726525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating models","metadata":{"papermill":{"duration":0.011542,"end_time":"2024-02-20T21:39:31.274543","exception":false,"start_time":"2024-02-20T21:39:31.263001","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"This code is copied from: https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py and modified.\nPlease refer to the original paper: https://arxiv.org/pdf/1703.10593.pdf","metadata":{}},{"cell_type":"markdown","source":"# Cycle-Consistency Loss in CycleGAN\n\nThe concept of cycle-consistency loss is a fundamental component of the CycleGAN architecture, which is designed for image-to-image translation tasks where paired examples are not available. This method enables the translation of images from one domain to another (e.g., horses to zebras, summer to winter scenes) without the need for corresponding image pairs in both domains.\n\n## Components of CycleGAN\n\nCycleGAN consists of two main types of networks:\n\n### Generators (G and F)\n\n- **G**: Maps from domain X to domain Y ($G: X \\rightarrow Y$).\n- **F**: Maps from domain Y to domain X ($F: Y \\rightarrow X$).\n\nThese generators are responsible for translating images from one domain to the other.\n\n### Discriminators (Dx and Dy)\n\n- **Dx**: Discriminates between images from domain X and translated images $F(Y)$.\n- **Dy**: Discriminates between images from domain Y and translated images $G(X)$.\n\nThe discriminators aim to distinguish real images from translated ones, helping to refine the generators.\n\n## Cycle-Consistency Loss\n\nThe cycle-consistency loss ensures that an image can undergo a round-trip translation (domain X to Y and back to X, or Y to X and back to Y) ending up similar to the original image. This is critical for learning meaningful translations without paired examples.\n\n### Forward Cycle\n\n1. An image from domain X is translated to domain Y using generator G to get $\\hat{Y}$.\n2. $\\hat{Y}$ is then translated back to domain X using generator F to get $\\tilde{X}$.\n3. The goal is for $\\tilde{X}$ to closely resemble the original image X.\n\n### Backward Cycle\n\n1. An image from domain Y is translated to domain X using generator F to get $\\hat{X}$.\n2. $\\hat{X}$ is then translated back to domain Y using generator G to get $\\tilde{Y}$.\n3. The goal is for $\\tilde{Y}$ to closely resemble the original image Y.\n\n### Loss Calculation\n\nThe cycle-consistency loss combines losses from both the forward and backward cycles, aiming to minimize the difference between the original and cycled images. This encourages the network to learn transformations that preserve content while changing domain-specific attributes.\n\nThis approach has been widely used for various image-to-image translation tasks, showcasing its effectiveness and versatility in unsupervised learning scenarios.","metadata":{}},{"cell_type":"markdown","source":"![cycle_losss](https://www.tensorflow.org/static/tutorials/generative/images/cycle_loss.png)","metadata":{}},{"cell_type":"code","source":"class InstanceNormalization(tf.keras.layers.Layer):\n  \"\"\"Instance Normalization Layer (https://arxiv.org/abs/1607.08022).\"\"\"\n\n  def __init__(self, epsilon=5e-5):\n    super(InstanceNormalization, self).__init__()\n    self.epsilon = epsilon\n\n  def build(self, input_shape):\n    self.scale = self.add_weight(\n        name='scale',\n        shape=input_shape[-1:],\n        initializer=tf.random_normal_initializer(1., 0.05),\n        trainable=True)\n\n    self.offset = self.add_weight(\n        name='offset',\n        shape=input_shape[-1:],\n        initializer='zeros',\n        trainable=True)\n\n  def call(self, x):\n    mean, variance = tf.nn.moments(x, axes=[1, 2], keepdims=True)\n    inv = tf.math.rsqrt(variance + self.epsilon)\n    normalized = (x - mean) * inv\n    return self.scale * normalized + self.offset","metadata":{"execution":{"iopub.status.busy":"2024-03-05T00:23:37.121958Z","iopub.execute_input":"2024-03-05T00:23:37.122578Z","iopub.status.idle":"2024-03-05T00:23:37.131336Z","shell.execute_reply.started":"2024-03-05T00:23:37.122545Z","shell.execute_reply":"2024-03-05T00:23:37.130318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def downsample(filters, size, norm_type='batchnorm', apply_norm=True):\n  \"\"\"Downsamples an input.\n\n  Conv2D => Batchnorm => LeakyRelu\n\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n    apply_norm: If True, adds the batchnorm layer\n\n  Returns:\n    Downsample Sequential Model\n  \"\"\"\n  initializer = tf.random_normal_initializer(0., 0.05)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n  if apply_norm:\n    if norm_type.lower() == 'batchnorm':\n      result.add(tf.keras.layers.BatchNormalization())\n    elif norm_type.lower() == 'instancenorm':\n      result.add(InstanceNormalization())\n\n  result.add(tf.keras.layers.LeakyReLU())\n\n  return result","metadata":{"execution":{"iopub.status.busy":"2024-03-05T00:24:12.366623Z","iopub.execute_input":"2024-03-05T00:24:12.367429Z","iopub.status.idle":"2024-03-05T00:24:12.377583Z","shell.execute_reply.started":"2024-03-05T00:24:12.367391Z","shell.execute_reply":"2024-03-05T00:24:12.376397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upsample(filters, size, norm_type='batchnorm', apply_dropout=False):\n  \"\"\"Upsamples an input.\n\n  Conv2DTranspose => Batchnorm => Dropout => Relu\n\n  Args:\n    filters: number of filters\n    size: filter size\n    norm_type: Normalization type; either 'batchnorm' or 'instancenorm'.\n    apply_dropout: If True, adds the dropout layer\n\n  Returns:\n    Upsample Sequential Model\n  \"\"\"\n\n  initializer = tf.random_normal_initializer(0., 0.05)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n  if norm_type.lower() == 'batchnorm':\n    result.add(tf.keras.layers.BatchNormalization())\n  elif norm_type.lower() == 'instancenorm':\n    result.add(InstanceNormalization())\n\n  if apply_dropout:\n    result.add(tf.keras.layers.Dropout(0.6))\n\n  result.add(tf.keras.layers.ReLU())\n\n  return result","metadata":{"execution":{"iopub.status.busy":"2024-03-05T00:25:04.306996Z","iopub.execute_input":"2024-03-05T00:25:04.308065Z","iopub.status.idle":"2024-03-05T00:25:04.320922Z","shell.execute_reply.started":"2024-03-05T00:25:04.308023Z","shell.execute_reply":"2024-03-05T00:25:04.319691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Discriminator(norm_type='batchnorm', target=True):\n  \"\"\"PatchGan discriminator model (https://arxiv.org/abs/1611.07004).\n\n  Args:\n    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n    target: Bool, indicating whether target image is an input or not.\n\n  Returns:\n    Discriminator model\n  \"\"\"\n\n  initializer = tf.random_normal_initializer(0., 0.05)\n\n  inp = tf.keras.layers.Input(shape=[None, None, 3], name='input_image')\n  x = inp\n\n  if target:\n    tar = tf.keras.layers.Input(shape=[None, None, 3], name='target_image')\n    x = tf.keras.layers.concatenate([inp, tar])  # (bs, 256, 256, channels*2)\n\n  down1 = downsample(64, 4, norm_type, False)(x)  # (bs, 128, 128, 64)\n  down2 = downsample(128, 4, norm_type)(down1)  # (bs, 64, 64, 128)\n  down3 = downsample(256, 4, norm_type)(down2)  # (bs, 32, 32, 256)\n\n  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (bs, 34, 34, 256)\n  conv = tf.keras.layers.Conv2D(\n      512, 4, strides=1, kernel_initializer=initializer,\n      use_bias=False)(zero_pad1)  # (bs, 31, 31, 512)\n\n  if norm_type.lower() == 'batchnorm':\n    norm1 = tf.keras.layers.BatchNormalization()(conv)\n  elif norm_type.lower() == 'instancenorm':\n    norm1 = InstanceNormalization()(conv)\n\n  leaky_relu = tf.keras.layers.LeakyReLU()(norm1)\n\n  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (bs, 33, 33, 512)\n\n  last = tf.keras.layers.Conv2D(\n      1, 4, strides=1,\n      kernel_initializer=initializer)(zero_pad2)  # (bs, 30, 30, 1)\n\n  if target:\n    return tf.keras.Model(inputs=[inp, tar], outputs=last)\n  else:\n    return tf.keras.Model(inputs=inp, outputs=last)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T00:35:19.346512Z","iopub.execute_input":"2024-03-05T00:35:19.346842Z","iopub.status.idle":"2024-03-05T00:35:19.358051Z","shell.execute_reply.started":"2024-03-05T00:35:19.346817Z","shell.execute_reply":"2024-03-05T00:35:19.356884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unet_generator(output_channels, norm_type='batchnorm'):\n  \"\"\"Modified u-net generator model (https://arxiv.org/abs/1611.07004).\n\n  Args:\n    output_channels: Output channels\n    norm_type: Type of normalization. Either 'batchnorm' or 'instancenorm'.\n\n  Returns:\n    Generator model\n  \"\"\"\n\n  down_stack = [\n      downsample(64, 4, norm_type, apply_norm=False),  # (bs, 128, 128, 64)\n      downsample(128, 4, norm_type),  # (bs, 64, 64, 128)\n      downsample(256, 4, norm_type),  # (bs, 32, 32, 256)\n      downsample(512, 4, norm_type),  # (bs, 16, 16, 512)\n      downsample(512, 4, norm_type),  # (bs, 8, 8, 512)\n      downsample(512, 4, norm_type),  # (bs, 4, 4, 512)\n      downsample(512, 4, norm_type),  # (bs, 2, 2, 512)\n      downsample(512, 4, norm_type),  # (bs, 1, 1, 512)\n  ]\n\n  up_stack = [\n      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 2, 2, 1024)\n      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 4, 4, 1024)\n      upsample(512, 4, norm_type, apply_dropout=True),  # (bs, 8, 8, 1024)\n      upsample(512, 4, norm_type),  # (bs, 16, 16, 1024)\n      upsample(256, 4, norm_type),  # (bs, 32, 32, 512)\n      upsample(128, 4, norm_type),  # (bs, 64, 64, 256)\n      upsample(64, 4, norm_type),  # (bs, 128, 128, 128)\n  ]\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n  last = tf.keras.layers.Conv2DTranspose(\n      output_channels, 4, strides=2,\n      padding='same', kernel_initializer=initializer,\n      activation='tanh')  # (bs, 256, 256, 3)\n\n  concat = tf.keras.layers.Concatenate()\n\n  inputs = tf.keras.layers.Input(shape=[None, None, 3])\n  x = inputs\n\n  # Downsampling through the model\n  skips = []\n  for down in down_stack:\n    x = down(x)\n    skips.append(x)\n\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    x = concat([x, skip])\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"execution":{"iopub.status.busy":"2024-03-05T00:37:45.986616Z","iopub.execute_input":"2024-03-05T00:37:45.987321Z","iopub.status.idle":"2024-03-05T00:37:45.998583Z","shell.execute_reply.started":"2024-03-05T00:37:45.987291Z","shell.execute_reply":"2024-03-05T00:37:45.997598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3\n# generator_g - takes a photo and tries to generate a monet\ngenerator_g = unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n# generator_f - takes a monet and tries to generate a photo\ngenerator_f = unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n\n#discriminator_x - estimating generator_g , gives feedback to generator, so that the generator can improve\ndiscriminator_x = Discriminator(norm_type='instancenorm', target=False)\n# discriminator_y - estimating generator_g , gives feedback to generator, so that the generator can improve\ndiscriminator_y = Discriminator(norm_type='instancenorm', target=False)","metadata":{"papermill":{"duration":2.061515,"end_time":"2024-02-20T21:39:33.347813","exception":false,"start_time":"2024-02-20T21:39:31.286298","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:39:35.257714Z","iopub.execute_input":"2024-03-05T00:39:35.258422Z","iopub.status.idle":"2024-03-05T00:39:36.268211Z","shell.execute_reply.started":"2024-03-05T00:39:35.258368Z","shell.execute_reply":"2024-03-05T00:39:36.267196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initial generation results (before training)\n\nto_monet = generator_g(sample_photo)\nplt.figure(figsize=(8, 8))\ncontrast = 8\n\nimgs = [sample_photo, to_monet]\ntitle = ['Photo', 'To Monet']\n\nfor i in range(len(imgs)):\n  plt.subplot(2, 2, i+1)\n  plt.title(title[i])\n  if i % 2 == 0:\n    plt.imshow(imgs[i][0] * 0.5 + 0.5)\n  else:\n    plt.imshow(imgs[i][0] * 0.5 * contrast + 0.5)\nplt.show()","metadata":{"papermill":{"duration":2.828405,"end_time":"2024-02-20T21:39:36.189062","exception":false,"start_time":"2024-02-20T21:39:33.360657","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:39:38.781952Z","iopub.execute_input":"2024-03-05T00:39:38.782846Z","iopub.status.idle":"2024-03-05T00:39:39.338646Z","shell.execute_reply.started":"2024-03-05T00:39:38.782812Z","shell.execute_reply":"2024-03-05T00:39:39.337719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LAMBDA = 10","metadata":{"papermill":{"duration":0.023561,"end_time":"2024-02-20T21:39:36.229875","exception":false,"start_time":"2024-02-20T21:39:36.206314","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:39:45.985976Z","iopub.execute_input":"2024-03-05T00:39:45.986607Z","iopub.status.idle":"2024-03-05T00:39:45.990704Z","shell.execute_reply.started":"2024-03-05T00:39:45.986575Z","shell.execute_reply":"2024-03-05T00:39:45.989803Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"papermill":{"duration":0.023792,"end_time":"2024-02-20T21:39:36.270079","exception":false,"start_time":"2024-02-20T21:39:36.246287","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:39:47.453556Z","iopub.execute_input":"2024-03-05T00:39:47.453953Z","iopub.status.idle":"2024-03-05T00:39:47.459497Z","shell.execute_reply.started":"2024-03-05T00:39:47.453921Z","shell.execute_reply":"2024-03-05T00:39:47.458420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def discriminator_loss(real, generated):\n  real_loss = loss_obj(tf.ones_like(real), real)\n\n  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss * 0.5","metadata":{"papermill":{"duration":0.023784,"end_time":"2024-02-20T21:39:36.310341","exception":false,"start_time":"2024-02-20T21:39:36.286557","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:39:50.678282Z","iopub.execute_input":"2024-03-05T00:39:50.679022Z","iopub.status.idle":"2024-03-05T00:39:50.684281Z","shell.execute_reply.started":"2024-03-05T00:39:50.678991Z","shell.execute_reply":"2024-03-05T00:39:50.683067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generator_loss(generated):\n  return loss_obj(tf.ones_like(generated), generated)","metadata":{"papermill":{"duration":0.023252,"end_time":"2024-02-20T21:39:36.350049","exception":false,"start_time":"2024-02-20T21:39:36.326797","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:39:53.188113Z","iopub.execute_input":"2024-03-05T00:39:53.189221Z","iopub.status.idle":"2024-03-05T00:39:53.193611Z","shell.execute_reply.started":"2024-03-05T00:39:53.189183Z","shell.execute_reply":"2024-03-05T00:39:53.192658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def calc_cycle_loss(real_image, cycled_image):\n  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n  return LAMBDA * loss1","metadata":{"papermill":{"duration":0.024507,"end_time":"2024-02-20T21:39:36.391458","exception":false,"start_time":"2024-02-20T21:39:36.366951","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:39:54.742956Z","iopub.execute_input":"2024-03-05T00:39:54.743359Z","iopub.status.idle":"2024-03-05T00:39:54.748417Z","shell.execute_reply.started":"2024-03-05T00:39:54.743328Z","shell.execute_reply":"2024-03-05T00:39:54.747330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def identity_loss(real_image, same_image):\n  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n  return LAMBDA * 0.5 * loss","metadata":{"papermill":{"duration":0.023503,"end_time":"2024-02-20T21:39:36.431517","exception":false,"start_time":"2024-02-20T21:39:36.408014","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:39:56.902639Z","iopub.execute_input":"2024-03-05T00:39:56.902977Z","iopub.status.idle":"2024-03-05T00:39:56.908398Z","shell.execute_reply.started":"2024-03-05T00:39:56.902951Z","shell.execute_reply":"2024-03-05T00:39:56.907403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"papermill":{"duration":0.031164,"end_time":"2024-02-20T21:39:36.479186","exception":false,"start_time":"2024-02-20T21:39:36.448022","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:40:00.274067Z","iopub.execute_input":"2024-03-05T00:40:00.274868Z","iopub.status.idle":"2024-03-05T00:40:00.286946Z","shell.execute_reply.started":"2024-03-05T00:40:00.274827Z","shell.execute_reply":"2024-03-05T00:40:00.285897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{"papermill":{"duration":0.016915,"end_time":"2024-02-20T21:39:36.512861","exception":false,"start_time":"2024-02-20T21:39:36.495946","status":"completed"},"tags":[]}},{"cell_type":"code","source":"EPOCHS = 100","metadata":{"papermill":{"duration":0.023231,"end_time":"2024-02-20T21:39:36.597834","exception":false,"start_time":"2024-02-20T21:39:36.574603","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:40:03.090077Z","iopub.execute_input":"2024-03-05T00:40:03.090449Z","iopub.status.idle":"2024-03-05T00:40:03.095018Z","shell.execute_reply.started":"2024-03-05T00:40:03.090418Z","shell.execute_reply":"2024-03-05T00:40:03.093921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for dinamic visualization\ndef generate_images(model, test_input):\n  prediction = model(test_input)\n\n  plt.figure(figsize=(12, 12))\n\n  display_list = [test_input[0], prediction[0]]\n  title = ['Input Image', 'Predicted Image']\n\n  for i in range(2):\n    plt.subplot(1, 2, i+1)\n    plt.title(title[i])\n    # getting the pixel values between [0, 1] to plot it.\n    plt.imshow(display_list[i] * 0.5 + 0.5)\n    plt.axis('off')\n  plt.show()","metadata":{"papermill":{"duration":0.025099,"end_time":"2024-02-20T21:39:36.639339","exception":false,"start_time":"2024-02-20T21:39:36.614240","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:40:05.364501Z","iopub.execute_input":"2024-03-05T00:40:05.365266Z","iopub.status.idle":"2024-03-05T00:40:05.371143Z","shell.execute_reply.started":"2024-03-05T00:40:05.365234Z","shell.execute_reply":"2024-03-05T00:40:05.370195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(real_x, real_y):\n  # persistent is set to True because the tape is used more than\n  # once to calculate the gradients.\n  with tf.GradientTape(persistent=True) as tape:\n    # Generator G translates X -> Y\n    # Generator F translates Y -> X.\n\n    fake_y = generator_g(real_x, training=True)\n    cycled_x = generator_f(fake_y, training=True)\n\n    fake_x = generator_f(real_y, training=True)\n    cycled_y = generator_g(fake_x, training=True)\n\n    # same_x and same_y are used for identity loss.\n    same_x = generator_f(real_x, training=True)\n    same_y = generator_g(real_y, training=True)\n\n    disc_real_x = discriminator_x(real_x, training=True)\n    disc_real_y = discriminator_y(real_y, training=True)\n\n    disc_fake_x = discriminator_x(fake_x, training=True)\n    disc_fake_y = discriminator_y(fake_y, training=True)\n\n    # calculate the loss\n    gen_g_loss = generator_loss(disc_fake_y)\n    gen_f_loss = generator_loss(disc_fake_x)\n\n    total_cycle_loss = calc_cycle_loss(real_y, cycled_y)\n\n    # Total generator loss = adversarial loss + cycle loss\n    total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n    total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n\n    disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n    disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n  # Calculate the gradients for generator and discriminator\n    generator_g_gradients = tape.gradient(total_gen_g_loss, \n                                        generator_g.trainable_variables)\n    generator_f_gradients = tape.gradient(total_gen_f_loss, \n                                        generator_f.trainable_variables)\n\n    discriminator_x_gradients = tape.gradient(disc_x_loss, \n                                            discriminator_x.trainable_variables)\n    discriminator_y_gradients = tape.gradient(disc_y_loss, \n                                            discriminator_y.trainable_variables)\n\n  # Apply the gradients to the optimizer\n    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, \n                                            generator_g.trainable_variables))\n\n    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, \n                                            generator_f.trainable_variables))\n\n    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n                                                discriminator_x.trainable_variables))\n\n    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n                                                discriminator_y.trainable_variables))","metadata":{"papermill":{"duration":0.030737,"end_time":"2024-02-20T21:39:36.686545","exception":false,"start_time":"2024-02-20T21:39:36.655808","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:45:43.545131Z","iopub.execute_input":"2024-03-05T00:45:43.545586Z","iopub.status.idle":"2024-03-05T00:45:43.558554Z","shell.execute_reply.started":"2024-03-05T00:45:43.545555Z","shell.execute_reply":"2024-03-05T00:45:43.557490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"let's train the Cycle GAN. For the visualization of each Epoch, we will use the same sample photo, so that we can see the progress of the Generator.","metadata":{}},{"cell_type":"code","source":"for epoch in range(EPOCHS):\n  start = time.time()\n\n  n = 0\n  for image_x, image_y in tf.data.Dataset.zip((train_photo, train_monet)):\n    train_step(image_x, image_y)\n    if n % 10 == 0:\n      print ('.', end='')\n    n += 1\n\n  clear_output(wait=True).\n  generate_images(generator_g, sample_photo)\n\n  print (f'Time taken for epoch {(epoch + 1)} is {(time.time()-start)} sec\\n')","metadata":{"papermill":{"duration":2882.873279,"end_time":"2024-02-20T22:27:39.576469","exception":false,"start_time":"2024-02-20T21:39:36.703190","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-05T00:45:56.822103Z","iopub.execute_input":"2024-03-05T00:45:56.822475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizing results","metadata":{"papermill":{"duration":0.023775,"end_time":"2024-02-20T22:27:39.625525","exception":false,"start_time":"2024-02-20T22:27:39.601750","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Run the trained model on the test dataset\nfor inp in test_photo.skip(5).take(5):\n  generate_images(generator_g, inp)","metadata":{"papermill":{"duration":2.882599,"end_time":"2024-02-20T22:27:42.531501","exception":false,"start_time":"2024-02-20T22:27:39.648902","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-03-04T23:34:29.496523Z","iopub.execute_input":"2024-03-04T23:34:29.496882Z","iopub.status.idle":"2024-03-04T23:34:32.498948Z","shell.execute_reply.started":"2024-03-04T23:34:29.496855Z","shell.execute_reply":"2024-03-04T23:34:32.498015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving results","metadata":{"papermill":{"duration":0.072029,"end_time":"2024-02-20T22:27:42.676008","exception":false,"start_time":"2024-02-20T22:27:42.603979","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import PIL\n!mkdir /kaggle/working/images\nimport numpy as np","metadata":{"execution":{"iopub.execute_input":"2024-02-20T22:27:42.820909Z","iopub.status.busy":"2024-02-20T22:27:42.820559Z","iopub.status.idle":"2024-02-20T22:27:43.812260Z","shell.execute_reply":"2024-02-20T22:27:43.810899Z"},"papermill":{"duration":1.06725,"end_time":"2024-02-20T22:27:43.814905","exception":false,"start_time":"2024-02-20T22:27:42.747655","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i = 1\nfor image in photo_data:\n    pred = generator_g(image, training=False)[0].numpy()\n    pred = (pred*127.5 + 127.5).astype(np.uint8)\n    im = PIL.Image.fromarray(pred)\n    im.save(\"/kaggle/working/images/generated_\" + str(i) + \".jpg\")\n    i += 1","metadata":{"execution":{"iopub.execute_input":"2024-02-20T22:27:43.962878Z","iopub.status.busy":"2024-02-20T22:27:43.961970Z","iopub.status.idle":"2024-02-20T22:33:44.821008Z","shell.execute_reply":"2024-02-20T22:33:44.819975Z"},"papermill":{"duration":360.935009,"end_time":"2024-02-20T22:33:44.823613","exception":false,"start_time":"2024-02-20T22:27:43.888604","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/working/images\")","metadata":{"execution":{"iopub.execute_input":"2024-02-20T22:33:44.980062Z","iopub.status.busy":"2024-02-20T22:33:44.979721Z","iopub.status.idle":"2024-02-20T22:33:49.636728Z","shell.execute_reply":"2024-02-20T22:33:49.635733Z"},"papermill":{"duration":4.737289,"end_time":"2024-02-20T22:33:49.638903","exception":false,"start_time":"2024-02-20T22:33:44.901614","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}